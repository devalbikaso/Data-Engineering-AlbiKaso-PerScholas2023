# Data-Engineering-AlbiKaso-PerScholas2023


In my ETL (Extract, Transform, Load) process, I started by extracting data from JSON files, which served as the source data. Using Python, I utilized libraries like json to read and parse the JSON files, transforming them into structured data. Additionally, I created two separate console-based Python scripts for managing a loan and a credit card line, allowing for streamlined data entry and calculation of loan and credit-related parameters.

Next, I established a connection to a MySQL database to load the transformed data, ensuring a reliable and structured storage platform. I employed PySpark and Pandas for data analytics and visualization. PySpark enabled me to process large datasets efficiently, and Pandas facilitated data analysis and visualization with its rich set of tools. I performed data analysis, cleaning, and transformations, as well as generated visualizations using Matplotlib or other Python plotting libraries to gain insights and effectively communicate results.

This comprehensive ETL process allowed me to extract, transform, load, analyze, and visualize data from JSON files to draw meaningful insights and make data-driven decisions using MySQL, PySpark, and Pandas, while the additional Python scripts enhanced functionality for managing loan and credit card information.
